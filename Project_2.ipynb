{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NNIA 18/19 Project 2:  Gradient Descent & Backpropagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deadline: 4. January 2018, 23:59"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Multinomial Logistic Regression and Cross Validation $~$ (12 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this exercise, you will implement a [multinomial logistic regression](https://en.wikipedia.org/wiki/Multinomial_logistic_regression) model with tensorflow for Fashion-MNIST dataset. Cross Validation will be used to find the best **regularization parameter** $\\lambda$ for the L2-regularization term. Fashion-MNIST dataset is similar to the sklearn Digit dataset you used in the Project 1. It contains 60,000 training images and 10,000 testing images. Each example is a 28×28 grayscale image, associated with a label from 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Neural Network](https://s3-eu-central-1.amazonaws.com/zalando-wp-zalando-research-production/2017/08/fashion-mnist-sprite.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multinomial logistic regression is a probabilistic, linear classifier. It is parametrized by a weight matrix $W$ and a bias vector $b$. Classification is done by projecting an input vector onto a set of hyperplanes, each of which corresponds to a class. The distance from the input to a hyperplane reflects the probability that the input is a member of the corresponding class.\n",
    "\n",
    "Mathematically, the probability that an input vector $\\bf{x} \\in \\mathbb{R}^p$ is a member of a class $i$ can be written as:\n",
    "$$P(Y=i|\\textbf{x}, W, b) = softmax(W\\textbf{x} + b)_i = \\frac{e^{W_i\\textbf{x} + b_i}}{\\sum_j{e^{W_j\\textbf{x} + b_j}}}$$\n",
    "where $W \\in \\mathbb{R}^{c \\times p}$, $b \\in \\mathbb{R}^c$ and $W_i \\in \\mathbb{R}^p$.\n",
    "\n",
    "The model’s prediction $y_{pred}$ is the class whose probability is maximal, specifically:\n",
    "$$y_{pred} = argmax_iP(Y=i|\\textbf{x}, W, b)$$\n",
    "\n",
    "We use cross-entropy loss with L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Dataset and Normalization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load **Fashion-MNIST** dataset and normalized it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
    "(X_trainval, Y_trainval), (X_test, Y_test) = fashion_mnist.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_trainval has the following shape:\n",
      "Rows: 60000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_trainval = np.reshape(X_trainval, (X_trainval.shape[0], X_trainval.shape[1] * X_trainval.shape[2]))\n",
    "print('The X_trainval has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_test has the following shape:\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "X_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1] * X_test.shape[2]))\n",
    "print('The X_test has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize the data. Subtract the mean and divide by the standard deviation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_normalization(X_trainval, X_test):\n",
    "    # TODO: Implement\n",
    "    # subtract the mean\n",
    "    mean = np.mean(X_trainval, axis=0)\n",
    "    X_trainval = (X_trainval - mean)\n",
    "    # standard deviation\n",
    "    std = np.std(X_trainval, axis=0)\n",
    "    # no zero std index\n",
    "    idx = std != 0\n",
    "    X_trainval_normalized = X_trainval[:, idx] / std[idx]\n",
    "\n",
    "    # subtract the mean\n",
    "    X_test = (X_test - mean)\n",
    "    X_test_normalized = X_test[:, idx] / std[idx]\n",
    "    return X_trainval_normalized, X_test_normalized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The X_trainval has the following shape:\n",
      "Rows: 60000, columns: 784\n",
      "The X_test has the following shape:\n",
      "Rows: 10000, columns: 784\n"
     ]
    }
   ],
   "source": [
    "# The normalization should be done on X_train and X_test. \n",
    "# The normalized data should have the exactly same shape as the original data matrix.\n",
    "\n",
    "X_trainval, X_test = data_normalization(X_trainval, X_test)\n",
    "print('The X_trainval has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_trainval.shape[0], X_trainval.shape[1]))\n",
    "print('The X_test has the following shape:')\n",
    "print('Rows: %d, columns: %d' % (X_test.shape[0], X_test.shape[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Define the Computation Graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here the global configuration of this program is \n",
    "# defined, which you shouldn't change.\n",
    "\n",
    "class global_config(object):\n",
    "    lr = 0.0001  # learning rate\n",
    "    img_h = 28  # image height\n",
    "    img_w = 28  # image width\n",
    "    num_class = 10  # number of classes\n",
    "    num_epoch = 20  # number of training epochs\n",
    "    batch_size = 16  # batch size\n",
    "    K = 3  # K-fold cross validation\n",
    "    num_train = None  # the number of training data\n",
    "    lambd = None  # the factor for the L2-regularization\n",
    "\n",
    "config = global_config()\n",
    "config.num_train = X_trainval.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_val_split(X_trainval, Y_trainval, i, K):\n",
    "    \"\"\"\n",
    "    sklearn library is not allowed to use here.\n",
    "    \n",
    "    K is the total number of folds and i is the current fold.\n",
    "    \n",
    "    Think about how to deal with the case when the number of \n",
    "    training data can't be divided by K evenly.\n",
    "    \"\"\"\n",
    "    # TODO: Implement\n",
    "    lens = np.ceil(X_trainval.shape[0] / K).astype(int)\n",
    "    begin_idx = i * lens\n",
    "    end_idx = min(X_trainval.shape[0], begin_idx + lens - 1)  # substract 1 due to idx start from 0\n",
    "    X_val = X_trainval[begin_idx:end_idx, :]\n",
    "    Y_val = Y_trainval[begin_idx:end_idx]\n",
    "    X_train = np.delete(X_trainval, np.linspace(begin_idx, end_idx, lens), axis=0)\n",
    "    Y_train = np.delete(Y_trainval, np.linspace(begin_idx, end_idx, lens), axis=0)\n",
    "    return X_train, X_val, Y_train, Y_val"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_train_data(X_train, Y_train):\n",
    "    \"\"\"called after each epoch\"\"\"\n",
    "    perm = np.random.permutation(len(Y_train))\n",
    "    Xtr_shuf = X_train[perm]\n",
    "    Ytr_shuf = Y_train[perm]\n",
    "    return Xtr_shuf, Ytr_shuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "training\n",
    "\"\"\"\n",
    "class logistic_regression(object):\n",
    "    \n",
    "    def __init__(self, X, Y_gt, config, name):\n",
    "        \"\"\"\n",
    "        :param X: the training batch, which has the shape [batch_size, n_features].\n",
    "        :param Y_gt: the corresponding ground truth label vector.\n",
    "        :param config: the hyper-parameters you need for the implementation.\n",
    "        :param name: the name of this logistic regression model which is used to\n",
    "                     avoid the naming confict with the help of tf.variable_scope and reuse.\n",
    "       \n",
    "        Define the computation graph within the variable_scope here. \n",
    "        First define two variables W and b with tf.get_variable.\n",
    "        Then do the forward pass.\n",
    "        Then compute the cross entropy loss with tensorflow, don't forget the L2-regularization.\n",
    "        The Adam optimizer is already given. You shouldn't change it.\n",
    "        Finally compute the accuracy for one batch\n",
    "        \"\"\"\n",
    "        self.config = config\n",
    "        with tf.variable_scope(name, reuse=tf.AUTO_REUSE):\n",
    "            # TODO: Define two variables and the forward pass.\n",
    "            self._w = tf.get_variable('w', [X.shape[1], config.num_class])\n",
    "            self._b = tf.get_variable('b', [config.num_class])\n",
    "\n",
    "            # forward pass\n",
    "            self._logits = tf.matmul(X, self._w) + self._b\n",
    "            # TODO: Compute the cross entropy loss with L2-regularization.\n",
    "            self.Y_gt_onehot = tf.one_hot(Y_gt, self.config.num_class, 1, 0)\n",
    "            self._loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(labels=self.Y_gt_onehot,\n",
    "                                                                 logits=self._logits)) + self.config.lambd * tf.nn.l2_loss(self._w)\n",
    "            # Adam is an optimization algorithm that can be used instead of the classical stochastic gradient descent\n",
    "            # to update network weights iteratively.\n",
    "            # It will be introduced in the lecture when talking about the optimization algorithms.\n",
    "            self._train_step = tf.train.AdamOptimizer(config.lr).minimize(self._loss)\n",
    "\n",
    "            # TODO: Compute the accuracy\n",
    "            self._predict = tf.argmax(self._logits, 1)\n",
    "            self._num_acc = tf.reduce_sum(tf.cast(tf.equal(self._predict, Y_gt), tf.float32))\n",
    "\n",
    "            \n",
    "    @property\n",
    "    def train_op(self):\n",
    "        return self._train_step\n",
    "    \n",
    "    @property\n",
    "    def loss(self):\n",
    "        return self._loss\n",
    "    \n",
    "    @property\n",
    "    def num_acc(self):\n",
    "        return self._num_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def testing(model, X_test, Y_test, config):\n",
    "    \"\"\" \n",
    "    Go through the X_test and use sess.run() to compute the loss and accuracy.\n",
    "    \n",
    "    Return the total loss and the accuracy for X_test.\n",
    "    \n",
    "    Note that this function will be used for the validation data\n",
    "    during training and the test data after training.\n",
    "    \"\"\"\n",
    "    num_test = X_test.shape[0]\n",
    "    total_cost = 0\n",
    "    accs = 0\n",
    "    # TODO: Implement\n",
    "    total_cost = sess.run(model.loss, feed_dict={X: X_test, Y_gt: Y_test})\n",
    "    accs = sess.run(model.num_acc, feed_dict={X: X_test, Y_gt: Y_test})\n",
    "    return total_cost / len(Y_test), accs / len(Y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, X_train, X_val, Y_train, Y_val, config):\n",
    "    \"\"\"\n",
    "    Train the model with sess.run().\n",
    "    \n",
    "    You should shuffle the data after each epoch and\n",
    "    evaluate training and validation loss after each epoch.\n",
    "    \n",
    "    Return the lists of the training/validation loss and accuracy.\n",
    "    \"\"\"\n",
    "    cost_trains = []\n",
    "    acc_trains = []\n",
    "    cost_vals = []\n",
    "    acc_vals = []\n",
    "    \n",
    "    for i in range(config.num_epoch):\n",
    "        # TODO: Implement\n",
    "        #with tf.Session() as sess:\n",
    "        # Initialize the variables of the model\n",
    "        # sess.run(tf.global_variables_initializer())???\n",
    "        sess.run(model.train_op, feed_dict={X: X_train, Y_gt: Y_train})\n",
    "        cost_train = sess.run(model.loss, feed_dict={X: X_train, Y_gt: Y_train}) / len(Y_train)  # average cost\n",
    "        acc_train = sess.run(model.num_acc, feed_dict={X: X_train, Y_gt: Y_train}) / len(Y_train)  # average accuracy\n",
    "\n",
    "        cost_trains.append(cost_train)\n",
    "        acc_trains.append(acc_train)\n",
    "        print(\"Epoch: %d :\" % (i + 1))\n",
    "        print(\"Train Loss: %f\" % cost_train)\n",
    "        print(\"Training acc: %f\" % acc_train)\n",
    "\n",
    "        cost_val, acc_val = testing(model, X_val, Y_val, config)\n",
    "        cost_vals.append(cost_val)\n",
    "        acc_vals.append(acc_val)\n",
    "        print(\"Validation Loss: %f\" % cost_val)\n",
    "        print(\"Validation acc: %f\" % acc_val)\n",
    "        ## shuffle the data\n",
    "        X_train, Y_train = shuffle_train_data(X_train, Y_train)\n",
    "    return cost_trains, acc_trains, cost_vals, acc_vals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Cross Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement cross validation to find an optimal value of $\\lambda$. The optimal hyper-parameters should be determined by the validation accuracy. The test set should only be used in the very end after all other processing, e.g. hyper-parameter choosing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lambda is 100.000000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Lenovo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:16: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n",
      "  app.launch_new_instance()\n",
      "C:\\Users\\Lenovo\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\ipykernel_launcher.py:17: DeprecationWarning: using a non-integer array as obj in delete will result in an error in the future\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-633f8aa41d46>:32: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n",
      "Epoch: 1 :\n",
      "Train Loss: 2.877019\n",
      "Training acc: 0.106475\n",
      "Validation Loss: 2.912482\n",
      "Validation acc: 0.104455\n",
      "Epoch: 2 :\n",
      "Train Loss: 2.836704\n",
      "Training acc: 0.112150\n",
      "Validation Loss: 2.872011\n",
      "Validation acc: 0.110656\n",
      "Epoch: 3 :\n",
      "Train Loss: 2.796931\n",
      "Training acc: 0.119550\n",
      "Validation Loss: 2.832082\n",
      "Validation acc: 0.116956\n",
      "Epoch: 4 :\n",
      "Train Loss: 2.757710\n",
      "Training acc: 0.126200\n",
      "Validation Loss: 2.792709\n",
      "Validation acc: 0.122556\n",
      "Epoch: 5 :\n",
      "Train Loss: 2.719054\n",
      "Training acc: 0.133200\n",
      "Validation Loss: 2.753901\n",
      "Validation acc: 0.128956\n",
      "Epoch: 6 :\n",
      "Train Loss: 2.680970\n",
      "Training acc: 0.140400\n",
      "Validation Loss: 2.715669\n",
      "Validation acc: 0.135007\n",
      "Epoch: 7 :\n",
      "Train Loss: 2.643469\n",
      "Training acc: 0.147350\n",
      "Validation Loss: 2.678022\n",
      "Validation acc: 0.142257\n",
      "Epoch: 8 :\n",
      "Train Loss: 2.606558\n",
      "Training acc: 0.154725\n",
      "Validation Loss: 2.640968\n",
      "Validation acc: 0.150058\n",
      "Epoch: 9 :\n",
      "Train Loss: 2.570244\n",
      "Training acc: 0.161525\n",
      "Validation Loss: 2.604513\n",
      "Validation acc: 0.156458\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.534532\n",
      "Training acc: 0.169075\n",
      "Validation Loss: 2.568663\n",
      "Validation acc: 0.163308\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.499427\n",
      "Training acc: 0.176425\n",
      "Validation Loss: 2.533422\n",
      "Validation acc: 0.171509\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.464931\n",
      "Training acc: 0.184175\n",
      "Validation Loss: 2.498796\n",
      "Validation acc: 0.179409\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.431049\n",
      "Training acc: 0.192825\n",
      "Validation Loss: 2.464785\n",
      "Validation acc: 0.187409\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.397780\n",
      "Training acc: 0.201000\n",
      "Validation Loss: 2.431393\n",
      "Validation acc: 0.196410\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.365125\n",
      "Training acc: 0.208875\n",
      "Validation Loss: 2.398620\n",
      "Validation acc: 0.204610\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.333084\n",
      "Training acc: 0.216625\n",
      "Validation Loss: 2.366465\n",
      "Validation acc: 0.212761\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.301656\n",
      "Training acc: 0.224775\n",
      "Validation Loss: 2.334928\n",
      "Validation acc: 0.219911\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.270839\n",
      "Training acc: 0.233125\n",
      "Validation Loss: 2.304007\n",
      "Validation acc: 0.228561\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.240630\n",
      "Training acc: 0.241500\n",
      "Validation Loss: 2.273699\n",
      "Validation acc: 0.236512\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.211026\n",
      "Training acc: 0.249875\n",
      "Validation Loss: 2.244003\n",
      "Validation acc: 0.245012\n",
      "Epoch: 1 :\n",
      "Train Loss: 3.502539\n",
      "Training acc: 0.056800\n",
      "Validation Loss: 3.527712\n",
      "Validation acc: 0.056003\n",
      "Epoch: 2 :\n",
      "Train Loss: 3.460945\n",
      "Training acc: 0.059900\n",
      "Validation Loss: 3.486086\n",
      "Validation acc: 0.058603\n",
      "Epoch: 3 :\n",
      "Train Loss: 3.419807\n",
      "Training acc: 0.062775\n",
      "Validation Loss: 3.444912\n",
      "Validation acc: 0.061853\n",
      "Epoch: 4 :\n",
      "Train Loss: 3.379134\n",
      "Training acc: 0.065825\n",
      "Validation Loss: 3.404201\n",
      "Validation acc: 0.064703\n",
      "Epoch: 5 :\n",
      "Train Loss: 3.338936\n",
      "Training acc: 0.068975\n",
      "Validation Loss: 3.363962\n",
      "Validation acc: 0.068653\n",
      "Epoch: 6 :\n",
      "Train Loss: 3.299223\n",
      "Training acc: 0.072675\n",
      "Validation Loss: 3.324204\n",
      "Validation acc: 0.072004\n",
      "Epoch: 7 :\n",
      "Train Loss: 3.260004\n",
      "Training acc: 0.076200\n",
      "Validation Loss: 3.284936\n",
      "Validation acc: 0.075554\n",
      "Epoch: 8 :\n",
      "Train Loss: 3.221285\n",
      "Training acc: 0.080300\n",
      "Validation Loss: 3.246166\n",
      "Validation acc: 0.079154\n",
      "Epoch: 9 :\n",
      "Train Loss: 3.183073\n",
      "Training acc: 0.084950\n",
      "Validation Loss: 3.207900\n",
      "Validation acc: 0.082904\n",
      "Epoch: 10 :\n",
      "Train Loss: 3.145375\n",
      "Training acc: 0.088975\n",
      "Validation Loss: 3.170145\n",
      "Validation acc: 0.086904\n",
      "Epoch: 11 :\n",
      "Train Loss: 3.108194\n",
      "Training acc: 0.092900\n",
      "Validation Loss: 3.132906\n",
      "Validation acc: 0.091555\n",
      "Epoch: 12 :\n",
      "Train Loss: 3.071536\n",
      "Training acc: 0.096850\n",
      "Validation Loss: 3.096186\n",
      "Validation acc: 0.096955\n",
      "Epoch: 13 :\n",
      "Train Loss: 3.035403\n",
      "Training acc: 0.101775\n",
      "Validation Loss: 3.059991\n",
      "Validation acc: 0.101855\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.999798\n",
      "Training acc: 0.107025\n",
      "Validation Loss: 3.024322\n",
      "Validation acc: 0.106755\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.964722\n",
      "Training acc: 0.113025\n",
      "Validation Loss: 2.989181\n",
      "Validation acc: 0.112556\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.930176\n",
      "Training acc: 0.118400\n",
      "Validation Loss: 2.954568\n",
      "Validation acc: 0.119256\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.896159\n",
      "Training acc: 0.124275\n",
      "Validation Loss: 2.920484\n",
      "Validation acc: 0.124456\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.862670\n",
      "Training acc: 0.130250\n",
      "Validation Loss: 2.886928\n",
      "Validation acc: 0.129656\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.829708\n",
      "Training acc: 0.136300\n",
      "Validation Loss: 2.853897\n",
      "Validation acc: 0.135457\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.797269\n",
      "Training acc: 0.141925\n",
      "Validation Loss: 2.821388\n",
      "Validation acc: 0.141507\n",
      "Epoch: 1 :\n",
      "Train Loss: 2.850412\n",
      "Training acc: 0.082325\n",
      "Validation Loss: 2.886237\n",
      "Validation acc: 0.077904\n",
      "Epoch: 2 :\n",
      "Train Loss: 2.811468\n",
      "Training acc: 0.087600\n",
      "Validation Loss: 2.847106\n",
      "Validation acc: 0.083854\n",
      "Epoch: 3 :\n",
      "Train Loss: 2.773171\n",
      "Training acc: 0.094025\n",
      "Validation Loss: 2.808627\n",
      "Validation acc: 0.089454\n",
      "Epoch: 4 :\n",
      "Train Loss: 2.735531\n",
      "Training acc: 0.100175\n",
      "Validation Loss: 2.770809\n",
      "Validation acc: 0.095505\n",
      "Epoch: 5 :\n",
      "Train Loss: 2.698559\n",
      "Training acc: 0.106200\n",
      "Validation Loss: 2.733663\n",
      "Validation acc: 0.102205\n",
      "Epoch: 6 :\n",
      "Train Loss: 2.662262\n",
      "Training acc: 0.112950\n",
      "Validation Loss: 2.697196\n",
      "Validation acc: 0.108505\n",
      "Epoch: 7 :\n",
      "Train Loss: 2.626650\n",
      "Training acc: 0.119900\n",
      "Validation Loss: 2.661416\n",
      "Validation acc: 0.115456\n",
      "Epoch: 8 :\n",
      "Train Loss: 2.591725\n",
      "Training acc: 0.127300\n",
      "Validation Loss: 2.626328\n",
      "Validation acc: 0.122156\n",
      "Epoch: 9 :\n",
      "Train Loss: 2.557494\n",
      "Training acc: 0.134450\n",
      "Validation Loss: 2.591934\n",
      "Validation acc: 0.129556\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.523956\n",
      "Training acc: 0.142700\n",
      "Validation Loss: 2.558238\n",
      "Validation acc: 0.137207\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.491112\n",
      "Training acc: 0.150225\n",
      "Validation Loss: 2.525238\n",
      "Validation acc: 0.145057\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.458962\n",
      "Training acc: 0.158575\n",
      "Validation Loss: 2.492933\n",
      "Validation acc: 0.153558\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.427499\n",
      "Training acc: 0.165800\n",
      "Validation Loss: 2.461321\n",
      "Validation acc: 0.161108\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.396722\n",
      "Training acc: 0.174650\n",
      "Validation Loss: 2.430394\n",
      "Validation acc: 0.168408\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.366621\n",
      "Training acc: 0.183125\n",
      "Validation Loss: 2.400149\n",
      "Validation acc: 0.177559\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.337192\n",
      "Training acc: 0.191925\n",
      "Validation Loss: 2.370577\n",
      "Validation acc: 0.186859\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.308424\n",
      "Training acc: 0.199425\n",
      "Validation Loss: 2.341669\n",
      "Validation acc: 0.195560\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.280309\n",
      "Training acc: 0.207825\n",
      "Validation Loss: 2.313415\n",
      "Validation acc: 0.205160\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.252834\n",
      "Training acc: 0.215500\n",
      "Validation Loss: 2.285807\n",
      "Validation acc: 0.213111\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.225990\n",
      "Training acc: 0.223800\n",
      "Validation Loss: 2.258831\n",
      "Validation acc: 0.220961\n",
      "The validation loss for lambda 100.000000 is 2.441407\n",
      "lambda is 1.000000\n",
      "Epoch: 1 :\n",
      "Train Loss: 3.326559\n",
      "Training acc: 0.083400\n",
      "Validation Loss: 3.319211\n",
      "Validation acc: 0.084554\n",
      "Epoch: 2 :\n",
      "Train Loss: 3.287710\n",
      "Training acc: 0.087700\n",
      "Validation Loss: 3.280587\n",
      "Validation acc: 0.089754\n",
      "Epoch: 3 :\n",
      "Train Loss: 3.249339\n",
      "Training acc: 0.093525\n",
      "Validation Loss: 3.242450\n",
      "Validation acc: 0.094455\n",
      "Epoch: 4 :\n",
      "Train Loss: 3.211456\n",
      "Training acc: 0.099000\n",
      "Validation Loss: 3.204807\n",
      "Validation acc: 0.099905\n",
      "Epoch: 5 :\n",
      "Train Loss: 3.174065\n",
      "Training acc: 0.104150\n",
      "Validation Loss: 3.167664\n",
      "Validation acc: 0.105605\n",
      "Epoch: 6 :\n",
      "Train Loss: 3.137173\n",
      "Training acc: 0.109775\n",
      "Validation Loss: 3.131028\n",
      "Validation acc: 0.111156\n",
      "Epoch: 7 :\n",
      "Train Loss: 3.100783\n",
      "Training acc: 0.115500\n",
      "Validation Loss: 3.094902\n",
      "Validation acc: 0.117006\n",
      "Epoch: 8 :\n",
      "Train Loss: 3.064898\n",
      "Training acc: 0.121075\n",
      "Validation Loss: 3.059289\n",
      "Validation acc: 0.123356\n",
      "Epoch: 9 :\n",
      "Train Loss: 3.029521\n",
      "Training acc: 0.127175\n",
      "Validation Loss: 3.024191\n",
      "Validation acc: 0.129706\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.994651\n",
      "Training acc: 0.132825\n",
      "Validation Loss: 2.989607\n",
      "Validation acc: 0.135707\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.960288\n",
      "Training acc: 0.138850\n",
      "Validation Loss: 2.955536\n",
      "Validation acc: 0.141257\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.926431\n",
      "Training acc: 0.144275\n",
      "Validation Loss: 2.921977\n",
      "Validation acc: 0.147157\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.893077\n",
      "Training acc: 0.149400\n",
      "Validation Loss: 2.888926\n",
      "Validation acc: 0.152458\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 14 :\n",
      "Train Loss: 2.860222\n",
      "Training acc: 0.154700\n",
      "Validation Loss: 2.856380\n",
      "Validation acc: 0.156858\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.827863\n",
      "Training acc: 0.158800\n",
      "Validation Loss: 2.824334\n",
      "Validation acc: 0.161808\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.795995\n",
      "Training acc: 0.163325\n",
      "Validation Loss: 2.792783\n",
      "Validation acc: 0.167158\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.764614\n",
      "Training acc: 0.168150\n",
      "Validation Loss: 2.761721\n",
      "Validation acc: 0.171759\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.733714\n",
      "Training acc: 0.172800\n",
      "Validation Loss: 2.731143\n",
      "Validation acc: 0.176159\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.703289\n",
      "Training acc: 0.178300\n",
      "Validation Loss: 2.701042\n",
      "Validation acc: 0.181359\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.673333\n",
      "Training acc: 0.182150\n",
      "Validation Loss: 2.671412\n",
      "Validation acc: 0.185759\n",
      "Epoch: 1 :\n",
      "Train Loss: 3.633360\n",
      "Training acc: 0.038075\n",
      "Validation Loss: 3.650145\n",
      "Validation acc: 0.037152\n",
      "Epoch: 2 :\n",
      "Train Loss: 3.587719\n",
      "Training acc: 0.039750\n",
      "Validation Loss: 3.604565\n",
      "Validation acc: 0.039152\n",
      "Epoch: 3 :\n",
      "Train Loss: 3.542515\n",
      "Training acc: 0.041900\n",
      "Validation Loss: 3.559415\n",
      "Validation acc: 0.041052\n",
      "Epoch: 4 :\n",
      "Train Loss: 3.497759\n",
      "Training acc: 0.044075\n",
      "Validation Loss: 3.514708\n",
      "Validation acc: 0.043952\n",
      "Epoch: 5 :\n",
      "Train Loss: 3.453463\n",
      "Training acc: 0.046375\n",
      "Validation Loss: 3.470454\n",
      "Validation acc: 0.046202\n",
      "Epoch: 6 :\n",
      "Train Loss: 3.409638\n",
      "Training acc: 0.049025\n",
      "Validation Loss: 3.426665\n",
      "Validation acc: 0.048602\n",
      "Epoch: 7 :\n",
      "Train Loss: 3.366293\n",
      "Training acc: 0.051325\n",
      "Validation Loss: 3.383350\n",
      "Validation acc: 0.051153\n",
      "Epoch: 8 :\n",
      "Train Loss: 3.323437\n",
      "Training acc: 0.053800\n",
      "Validation Loss: 3.340520\n",
      "Validation acc: 0.054103\n",
      "Epoch: 9 :\n",
      "Train Loss: 3.281082\n",
      "Training acc: 0.056725\n",
      "Validation Loss: 3.298183\n",
      "Validation acc: 0.056203\n",
      "Epoch: 10 :\n",
      "Train Loss: 3.239233\n",
      "Training acc: 0.059425\n",
      "Validation Loss: 3.256346\n",
      "Validation acc: 0.058403\n",
      "Epoch: 11 :\n",
      "Train Loss: 3.197899\n",
      "Training acc: 0.061850\n",
      "Validation Loss: 3.215018\n",
      "Validation acc: 0.061353\n",
      "Epoch: 12 :\n",
      "Train Loss: 3.157086\n",
      "Training acc: 0.065025\n",
      "Validation Loss: 3.174205\n",
      "Validation acc: 0.063453\n",
      "Epoch: 13 :\n",
      "Train Loss: 3.116801\n",
      "Training acc: 0.068000\n",
      "Validation Loss: 3.133912\n",
      "Validation acc: 0.066453\n",
      "Epoch: 14 :\n",
      "Train Loss: 3.077049\n",
      "Training acc: 0.071225\n",
      "Validation Loss: 3.094145\n",
      "Validation acc: 0.069603\n",
      "Epoch: 15 :\n",
      "Train Loss: 3.037835\n",
      "Training acc: 0.074550\n",
      "Validation Loss: 3.054908\n",
      "Validation acc: 0.072704\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.999161\n",
      "Training acc: 0.077750\n",
      "Validation Loss: 3.016205\n",
      "Validation acc: 0.075804\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.961031\n",
      "Training acc: 0.081525\n",
      "Validation Loss: 2.978039\n",
      "Validation acc: 0.079454\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.923448\n",
      "Training acc: 0.085350\n",
      "Validation Loss: 2.940412\n",
      "Validation acc: 0.082254\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.886414\n",
      "Training acc: 0.089125\n",
      "Validation Loss: 2.903327\n",
      "Validation acc: 0.086254\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.849929\n",
      "Training acc: 0.092875\n",
      "Validation Loss: 2.866784\n",
      "Validation acc: 0.090255\n",
      "Epoch: 1 :\n",
      "Train Loss: 3.143603\n",
      "Training acc: 0.087950\n",
      "Validation Loss: 3.138884\n",
      "Validation acc: 0.086704\n",
      "Epoch: 2 :\n",
      "Train Loss: 3.103200\n",
      "Training acc: 0.090775\n",
      "Validation Loss: 3.098644\n",
      "Validation acc: 0.089254\n",
      "Epoch: 3 :\n",
      "Train Loss: 3.063259\n",
      "Training acc: 0.093300\n",
      "Validation Loss: 3.058864\n",
      "Validation acc: 0.092255\n",
      "Epoch: 4 :\n",
      "Train Loss: 3.023791\n",
      "Training acc: 0.096400\n",
      "Validation Loss: 3.019554\n",
      "Validation acc: 0.096155\n",
      "Epoch: 5 :\n",
      "Train Loss: 2.984807\n",
      "Training acc: 0.099775\n",
      "Validation Loss: 2.980726\n",
      "Validation acc: 0.099505\n",
      "Epoch: 6 :\n",
      "Train Loss: 2.946318\n",
      "Training acc: 0.102950\n",
      "Validation Loss: 2.942390\n",
      "Validation acc: 0.102855\n",
      "Epoch: 7 :\n",
      "Train Loss: 2.908335\n",
      "Training acc: 0.106625\n",
      "Validation Loss: 2.904557\n",
      "Validation acc: 0.106455\n",
      "Epoch: 8 :\n",
      "Train Loss: 2.870867\n",
      "Training acc: 0.111075\n",
      "Validation Loss: 2.867235\n",
      "Validation acc: 0.109805\n",
      "Epoch: 9 :\n",
      "Train Loss: 2.833923\n",
      "Training acc: 0.114850\n",
      "Validation Loss: 2.830433\n",
      "Validation acc: 0.113906\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.797512\n",
      "Training acc: 0.119175\n",
      "Validation Loss: 2.794158\n",
      "Validation acc: 0.116806\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.761638\n",
      "Training acc: 0.123675\n",
      "Validation Loss: 2.758418\n",
      "Validation acc: 0.121306\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.726310\n",
      "Training acc: 0.128425\n",
      "Validation Loss: 2.723218\n",
      "Validation acc: 0.125356\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.691531\n",
      "Training acc: 0.133375\n",
      "Validation Loss: 2.688563\n",
      "Validation acc: 0.129906\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.657306\n",
      "Training acc: 0.138000\n",
      "Validation Loss: 2.654457\n",
      "Validation acc: 0.134857\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.623637\n",
      "Training acc: 0.142700\n",
      "Validation Loss: 2.620901\n",
      "Validation acc: 0.139807\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.590526\n",
      "Training acc: 0.147825\n",
      "Validation Loss: 2.587899\n",
      "Validation acc: 0.144307\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.557974\n",
      "Training acc: 0.152275\n",
      "Validation Loss: 2.555451\n",
      "Validation acc: 0.149157\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.525980\n",
      "Training acc: 0.157250\n",
      "Validation Loss: 2.523557\n",
      "Validation acc: 0.154658\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.494545\n",
      "Training acc: 0.162175\n",
      "Validation Loss: 2.492216\n",
      "Validation acc: 0.159508\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.463665\n",
      "Training acc: 0.167575\n",
      "Validation Loss: 2.461427\n",
      "Validation acc: 0.164608\n",
      "The validation loss for lambda 1.000000 is 2.666541\n",
      "lambda is 0.100000\n",
      "Epoch: 1 :\n",
      "Train Loss: 3.494966\n",
      "Training acc: 0.105200\n",
      "Validation Loss: 3.511399\n",
      "Validation acc: 0.104205\n",
      "Epoch: 2 :\n",
      "Train Loss: 3.451640\n",
      "Training acc: 0.112150\n",
      "Validation Loss: 3.467933\n",
      "Validation acc: 0.112356\n",
      "Epoch: 3 :\n",
      "Train Loss: 3.408936\n",
      "Training acc: 0.119400\n",
      "Validation Loss: 3.425087\n",
      "Validation acc: 0.118856\n",
      "Epoch: 4 :\n",
      "Train Loss: 3.366866\n",
      "Training acc: 0.127075\n",
      "Validation Loss: 3.382875\n",
      "Validation acc: 0.127556\n",
      "Epoch: 5 :\n",
      "Train Loss: 3.325439\n",
      "Training acc: 0.134300\n",
      "Validation Loss: 3.341306\n",
      "Validation acc: 0.135107\n",
      "Epoch: 6 :\n",
      "Train Loss: 3.284661\n",
      "Training acc: 0.141700\n",
      "Validation Loss: 3.300386\n",
      "Validation acc: 0.142257\n",
      "Epoch: 7 :\n",
      "Train Loss: 3.244534\n",
      "Training acc: 0.148575\n",
      "Validation Loss: 3.260116\n",
      "Validation acc: 0.148807\n",
      "Epoch: 8 :\n",
      "Train Loss: 3.205057\n",
      "Training acc: 0.155400\n",
      "Validation Loss: 3.220496\n",
      "Validation acc: 0.154958\n",
      "Epoch: 9 :\n",
      "Train Loss: 3.166225\n",
      "Training acc: 0.162025\n",
      "Validation Loss: 3.181520\n",
      "Validation acc: 0.161608\n",
      "Epoch: 10 :\n",
      "Train Loss: 3.128029\n",
      "Training acc: 0.168600\n",
      "Validation Loss: 3.143179\n",
      "Validation acc: 0.168008\n",
      "Epoch: 11 :\n",
      "Train Loss: 3.090457\n",
      "Training acc: 0.174700\n",
      "Validation Loss: 3.105462\n",
      "Validation acc: 0.174209\n",
      "Epoch: 12 :\n",
      "Train Loss: 3.053496\n",
      "Training acc: 0.180625\n",
      "Validation Loss: 3.068352\n",
      "Validation acc: 0.179159\n",
      "Epoch: 13 :\n",
      "Train Loss: 3.017127\n",
      "Training acc: 0.186075\n",
      "Validation Loss: 3.031834\n",
      "Validation acc: 0.184559\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.981334\n",
      "Training acc: 0.191500\n",
      "Validation Loss: 2.995891\n",
      "Validation acc: 0.189909\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.946101\n",
      "Training acc: 0.196800\n",
      "Validation Loss: 2.960505\n",
      "Validation acc: 0.194360\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.911411\n",
      "Training acc: 0.201200\n",
      "Validation Loss: 2.925660\n",
      "Validation acc: 0.198160\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.877249\n",
      "Training acc: 0.205500\n",
      "Validation Loss: 2.891343\n",
      "Validation acc: 0.203910\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.843604\n",
      "Training acc: 0.210100\n",
      "Validation Loss: 2.857540\n",
      "Validation acc: 0.208110\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.810465\n",
      "Training acc: 0.214400\n",
      "Validation Loss: 2.824242\n",
      "Validation acc: 0.212011\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.777823\n",
      "Training acc: 0.219475\n",
      "Validation Loss: 2.791440\n",
      "Validation acc: 0.216361\n",
      "Epoch: 1 :\n",
      "Train Loss: 2.288522\n",
      "Training acc: 0.251125\n",
      "Validation Loss: 2.279908\n",
      "Validation acc: 0.253463\n",
      "Epoch: 2 :\n",
      "Train Loss: 2.259516\n",
      "Training acc: 0.257650\n",
      "Validation Loss: 2.251051\n",
      "Validation acc: 0.259713\n",
      "Epoch: 3 :\n",
      "Train Loss: 2.230987\n",
      "Training acc: 0.263675\n",
      "Validation Loss: 2.222671\n",
      "Validation acc: 0.266763\n",
      "Epoch: 4 :\n",
      "Train Loss: 2.202944\n",
      "Training acc: 0.270875\n",
      "Validation Loss: 2.194774\n",
      "Validation acc: 0.272264\n",
      "Epoch: 5 :\n",
      "Train Loss: 2.175395\n",
      "Training acc: 0.277475\n",
      "Validation Loss: 2.167369\n",
      "Validation acc: 0.277314\n",
      "Epoch: 6 :\n",
      "Train Loss: 2.148348\n",
      "Training acc: 0.283250\n",
      "Validation Loss: 2.140464\n",
      "Validation acc: 0.283664\n",
      "Epoch: 7 :\n",
      "Train Loss: 2.121809\n",
      "Training acc: 0.289575\n",
      "Validation Loss: 2.114063\n",
      "Validation acc: 0.290065\n",
      "Epoch: 8 :\n",
      "Train Loss: 2.095781\n",
      "Training acc: 0.296000\n",
      "Validation Loss: 2.088172\n",
      "Validation acc: 0.296165\n",
      "Epoch: 9 :\n",
      "Train Loss: 2.070269\n",
      "Training acc: 0.302650\n",
      "Validation Loss: 2.062793\n",
      "Validation acc: 0.303015\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.045274\n",
      "Training acc: 0.309100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 2.037929\n",
      "Validation acc: 0.309215\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.020798\n",
      "Training acc: 0.315125\n",
      "Validation Loss: 2.013581\n",
      "Validation acc: 0.314516\n",
      "Epoch: 12 :\n",
      "Train Loss: 1.996841\n",
      "Training acc: 0.320700\n",
      "Validation Loss: 1.989748\n",
      "Validation acc: 0.322216\n",
      "Epoch: 13 :\n",
      "Train Loss: 1.973400\n",
      "Training acc: 0.327325\n",
      "Validation Loss: 1.966429\n",
      "Validation acc: 0.328716\n",
      "Epoch: 14 :\n",
      "Train Loss: 1.950474\n",
      "Training acc: 0.333050\n",
      "Validation Loss: 1.943622\n",
      "Validation acc: 0.334667\n",
      "Epoch: 15 :\n",
      "Train Loss: 1.928059\n",
      "Training acc: 0.339050\n",
      "Validation Loss: 1.921322\n",
      "Validation acc: 0.341667\n",
      "Epoch: 16 :\n",
      "Train Loss: 1.906150\n",
      "Training acc: 0.345200\n",
      "Validation Loss: 1.899525\n",
      "Validation acc: 0.347267\n",
      "Epoch: 17 :\n",
      "Train Loss: 1.884740\n",
      "Training acc: 0.351825\n",
      "Validation Loss: 1.878225\n",
      "Validation acc: 0.353068\n",
      "Epoch: 18 :\n",
      "Train Loss: 1.863823\n",
      "Training acc: 0.358075\n",
      "Validation Loss: 1.857415\n",
      "Validation acc: 0.360418\n",
      "Epoch: 19 :\n",
      "Train Loss: 1.843392\n",
      "Training acc: 0.364375\n",
      "Validation Loss: 1.837087\n",
      "Validation acc: 0.366068\n",
      "Epoch: 20 :\n",
      "Train Loss: 1.823437\n",
      "Training acc: 0.370375\n",
      "Validation Loss: 1.817233\n",
      "Validation acc: 0.371419\n",
      "Epoch: 1 :\n",
      "Train Loss: 3.375279\n",
      "Training acc: 0.084350\n",
      "Validation Loss: 3.394340\n",
      "Validation acc: 0.081254\n",
      "Epoch: 2 :\n",
      "Train Loss: 3.334446\n",
      "Training acc: 0.089625\n",
      "Validation Loss: 3.353416\n",
      "Validation acc: 0.085654\n",
      "Epoch: 3 :\n",
      "Train Loss: 3.294043\n",
      "Training acc: 0.094400\n",
      "Validation Loss: 3.312922\n",
      "Validation acc: 0.090855\n",
      "Epoch: 4 :\n",
      "Train Loss: 3.254083\n",
      "Training acc: 0.099850\n",
      "Validation Loss: 3.272869\n",
      "Validation acc: 0.096605\n",
      "Epoch: 5 :\n",
      "Train Loss: 3.214579\n",
      "Training acc: 0.105375\n",
      "Validation Loss: 3.233269\n",
      "Validation acc: 0.102305\n",
      "Epoch: 6 :\n",
      "Train Loss: 3.175542\n",
      "Training acc: 0.110250\n",
      "Validation Loss: 3.194135\n",
      "Validation acc: 0.107555\n",
      "Epoch: 7 :\n",
      "Train Loss: 3.136983\n",
      "Training acc: 0.115725\n",
      "Validation Loss: 3.155476\n",
      "Validation acc: 0.113306\n",
      "Epoch: 8 :\n",
      "Train Loss: 3.098912\n",
      "Training acc: 0.120775\n",
      "Validation Loss: 3.117302\n",
      "Validation acc: 0.119256\n",
      "Epoch: 9 :\n",
      "Train Loss: 3.061338\n",
      "Training acc: 0.126425\n",
      "Validation Loss: 3.079622\n",
      "Validation acc: 0.124256\n",
      "Epoch: 10 :\n",
      "Train Loss: 3.024270\n",
      "Training acc: 0.131975\n",
      "Validation Loss: 3.042445\n",
      "Validation acc: 0.130457\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.987714\n",
      "Training acc: 0.137925\n",
      "Validation Loss: 3.005776\n",
      "Validation acc: 0.137007\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.951678\n",
      "Training acc: 0.143775\n",
      "Validation Loss: 2.969624\n",
      "Validation acc: 0.143257\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.916166\n",
      "Training acc: 0.150425\n",
      "Validation Loss: 2.933993\n",
      "Validation acc: 0.149407\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.881183\n",
      "Training acc: 0.156650\n",
      "Validation Loss: 2.898887\n",
      "Validation acc: 0.155858\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.846732\n",
      "Training acc: 0.162425\n",
      "Validation Loss: 2.864310\n",
      "Validation acc: 0.161208\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.812815\n",
      "Training acc: 0.168075\n",
      "Validation Loss: 2.830264\n",
      "Validation acc: 0.167008\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.779435\n",
      "Training acc: 0.174550\n",
      "Validation Loss: 2.796751\n",
      "Validation acc: 0.173209\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.746590\n",
      "Training acc: 0.180000\n",
      "Validation Loss: 2.763770\n",
      "Validation acc: 0.178409\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.714281\n",
      "Training acc: 0.186500\n",
      "Validation Loss: 2.731322\n",
      "Validation acc: 0.184309\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.682506\n",
      "Training acc: 0.192525\n",
      "Validation Loss: 2.699405\n",
      "Validation acc: 0.190410\n",
      "The validation loss for lambda 0.100000 is 2.436026\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Initialization\n",
    "\"\"\"\n",
    "# Use cross validation to choose the best lambda for the L2-regularization from the list below\n",
    "lambda_list = [100, 1, 0.1]\n",
    "\n",
    "\n",
    "X = tf.placeholder(tf.float32, [None, config.img_h * config.img_w])\n",
    "Y_gt = tf.placeholder(tf.int64, [None, ])\n",
    "\n",
    "for lambd in lambda_list:\n",
    "    val_loss_list = []\n",
    "    config.lambd = lambd\n",
    "    print(\"lambda is %f\" % lambd)\n",
    "    \n",
    "    for i in range(config.K):\n",
    "        # Prepare the training and validation data\n",
    "        X_train, X_val, Y_train, Y_val = train_val_split(X_trainval, Y_trainval, i, config.K)\n",
    "        \n",
    "        # For each lambda and K, we build a new model and train it from scratch\n",
    "        model = logistic_regression(X, Y_gt, config, name=str(lambd)+'_'+str(config.K))\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            \n",
    "            # Initialize the variables of the model\n",
    "            sess.run(tf.global_variables_initializer())\n",
    "            \n",
    "            # Train the model\n",
    "            cost_trains, acc_trains, cost_vals, acc_vals = train(model, X_train, X_val, Y_train, Y_val, config)\n",
    "            \n",
    "        val_loss_list.append(cost_vals[-1])\n",
    "        \n",
    "    print(\"The validation loss for lambda %f is %f\" % (lambd, np.mean(val_loss_list)))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Combine Train and Validation data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the hyper-parameters you choose from the cross validation to re-train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 :\n",
      "Train Loss: 2.925089\n",
      "Training acc: 0.088350\n",
      "Validation Loss: 2.951783\n",
      "Validation acc: 0.086800\n",
      "Epoch: 2 :\n",
      "Train Loss: 2.889350\n",
      "Training acc: 0.091467\n",
      "Validation Loss: 2.916056\n",
      "Validation acc: 0.091000\n",
      "Epoch: 3 :\n",
      "Train Loss: 2.854046\n",
      "Training acc: 0.095017\n",
      "Validation Loss: 2.880759\n",
      "Validation acc: 0.094800\n",
      "Epoch: 4 :\n",
      "Train Loss: 2.819185\n",
      "Training acc: 0.098283\n",
      "Validation Loss: 2.845899\n",
      "Validation acc: 0.098100\n",
      "Epoch: 5 :\n",
      "Train Loss: 2.784773\n",
      "Training acc: 0.102350\n",
      "Validation Loss: 2.811486\n",
      "Validation acc: 0.102300\n",
      "Epoch: 6 :\n",
      "Train Loss: 2.750820\n",
      "Training acc: 0.106100\n",
      "Validation Loss: 2.777527\n",
      "Validation acc: 0.106300\n",
      "Epoch: 7 :\n",
      "Train Loss: 2.717330\n",
      "Training acc: 0.110583\n",
      "Validation Loss: 2.744026\n",
      "Validation acc: 0.110200\n",
      "Epoch: 8 :\n",
      "Train Loss: 2.684308\n",
      "Training acc: 0.114967\n",
      "Validation Loss: 2.710989\n",
      "Validation acc: 0.115300\n",
      "Epoch: 9 :\n",
      "Train Loss: 2.651758\n",
      "Training acc: 0.119567\n",
      "Validation Loss: 2.678420\n",
      "Validation acc: 0.119300\n",
      "Epoch: 10 :\n",
      "Train Loss: 2.619683\n",
      "Training acc: 0.124250\n",
      "Validation Loss: 2.646321\n",
      "Validation acc: 0.123700\n",
      "Epoch: 11 :\n",
      "Train Loss: 2.588086\n",
      "Training acc: 0.129583\n",
      "Validation Loss: 2.614696\n",
      "Validation acc: 0.128500\n",
      "Epoch: 12 :\n",
      "Train Loss: 2.556968\n",
      "Training acc: 0.135133\n",
      "Validation Loss: 2.583545\n",
      "Validation acc: 0.133700\n",
      "Epoch: 13 :\n",
      "Train Loss: 2.526332\n",
      "Training acc: 0.140983\n",
      "Validation Loss: 2.552871\n",
      "Validation acc: 0.139100\n",
      "Epoch: 14 :\n",
      "Train Loss: 2.496176\n",
      "Training acc: 0.146550\n",
      "Validation Loss: 2.522674\n",
      "Validation acc: 0.144900\n",
      "Epoch: 15 :\n",
      "Train Loss: 2.466500\n",
      "Training acc: 0.152150\n",
      "Validation Loss: 2.492953\n",
      "Validation acc: 0.150100\n",
      "Epoch: 16 :\n",
      "Train Loss: 2.437305\n",
      "Training acc: 0.158150\n",
      "Validation Loss: 2.463710\n",
      "Validation acc: 0.155100\n",
      "Epoch: 17 :\n",
      "Train Loss: 2.408588\n",
      "Training acc: 0.164067\n",
      "Validation Loss: 2.434941\n",
      "Validation acc: 0.160200\n",
      "Epoch: 18 :\n",
      "Train Loss: 2.380348\n",
      "Training acc: 0.170733\n",
      "Validation Loss: 2.406647\n",
      "Validation acc: 0.166000\n",
      "Epoch: 19 :\n",
      "Train Loss: 2.352583\n",
      "Training acc: 0.177617\n",
      "Validation Loss: 2.378824\n",
      "Validation acc: 0.171500\n",
      "Epoch: 20 :\n",
      "Train Loss: 2.325291\n",
      "Training acc: 0.183817\n",
      "Validation Loss: 2.351471\n",
      "Validation acc: 0.177600\n",
      "The final test acc is 0.177600\n"
     ]
    }
   ],
   "source": [
    "config.lambd =   1.000000 #TODO: Choose the best lambda\n",
    "model = logistic_regression(X, Y_gt, config, name='trainval')\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_trains, acc_trains, cost_tests, acc_tests = train(model, X_trainval, X_test, Y_trainval, Y_test, config)\n",
    "\n",
    "print(\"The final test acc is %f\" % acc_tests[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. What is the impact of k in k-fold cross validation?\n",
    "\n",
    "2. What will happen to the training if you change the $\\lambda$ for L2-regularization?\n",
    "\n",
    "3. Why do we perform the gradient descent on a batch of the data rather than all of the data?\n",
    "\n",
    "4. Why does the loss increase, when the learning rate is too large?\n",
    "\n",
    "5. Do we apply L2-regularization for the bias $b$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Answer:* \n",
    "* 1. balance the bias and variance \n",
    "* 2. if we change the lambda for L2-regularization, the weights of leaset loss maybe change.\n",
    "* 3. if your dataset is large, each gradient descent step is too expensive \n",
    "* 4. too large of a learning reate causes drastic parameters updatas which lead to divergent from mimima, in this case the loss increase\n",
    "* 5. no"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $2.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Getting to know Back-Propagation in details $~$ (18 points)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following exercise you would build a **feed-forward network** from scratch using **only** Numpy. For this, you also have to implement **Back-propagation** in python. Additionally, this network should have the option of **L2 regularization** enabled within it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Before you start**: In this exercise you will implement a single hidden layer feedforward neural network. In case you are unfamiliar with the terminology and notation used here, please consult chapter 6 of the Deep Learning Book before you proceed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generally speaking, a feedword neural network with a single hidden layer can be represented by the following function $$ f(x;\\theta) = f^{(2)}(f^{(1)}(f^{(0)}(x)))$$ where $f^{(0)}(x)$ is the input layer, $f^{(1)}(.)$ is the so called hidden layer, and $f^{(2)}(.)$ is the ouput layer of the network. $\\theta$ represents the parameters of the network whose values will be learned during the training phase."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The network that you will implement in this exercise has the following layers:\n",
    "* $f^{(0)}(x) = \\mathbf{X}$, with $\\mathbf{X} \\in \\mathbb{R}^{b,p}$ where $b$ is the batch size and $p$ is the number of features.\n",
    "* $f^{(1)}(.) = \\sigma(\\mathbf{X} \\mathbf{W_1}+b_1)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, p}$, $\\mathbf{W_1} \\in \\mathbb{R}^{p,u_1}$, $\\textbf{b}_1 \\in \\mathbb{R}^{u_1}$ where $u_1$ is the number of **hidden units**. Additonally, $\\sigma(x) = \\frac{1}{1 + \\exp{(-x})}$ is the **sigmoid** function.\n",
    "* $f^{(2)}(.) = softmax(\\mathbf{X} \\mathbf{W_2}+b_2)$, with $\\mathbf{X} \\in \\mathbb{R}^{b, u_1}$, $\\mathbf{W_2} \\in \\mathbb{R}^{u_1,u_2}$, $\\textbf{b}_2 \\in \\mathbb{R}^{u_2}$ where $u_2$ is the number of **output classes** in this particular layer.\n",
    "\n",
    "Note that both, $\\sigma(x)$ are applied **elementwise**. Further, the addition with the bias vector is also applied **elementwise** to each row of the matrix $\\mathbf{X} \\mathbf{W}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "class Fully_connected_Neural_Network(object):\n",
    "    \"\"\" Fully-connected neural network with one hidden layer.\n",
    "\n",
    "    Parameters\n",
    "    ------------\n",
    "    n_output : int\n",
    "        Number of class labels.\n",
    "        \n",
    "    n_features : int\n",
    "        Number of input features.\n",
    "        \n",
    "    n_hidden : int\n",
    "        Number of hidden units.\n",
    "        \n",
    "    l2 : float\n",
    "        regularization parameter\n",
    "        0 means no regularization\n",
    "        \n",
    "    epochs : int\n",
    "        One Epoch is when the entire dataset is passed forward and backward through the neural network only once.\n",
    "        \n",
    "    lr : float\n",
    "        Learning rate.\n",
    "        \n",
    "    batchsize : int\n",
    "        Total number of training examples present in a single batch.\n",
    "        \n",
    "\n",
    "    Attributes\n",
    "    -----------\n",
    "    w1 : array, shape = [n_features, n_hidden_units]\n",
    "        Weight matrix for input layer -> hidden layer.\n",
    "    w2 : array, shape = [n_hidden_units, n_output_units]\n",
    "        Weight matrix for hidden layer -> output layer.\n",
    "    b1 : array, shape = [n_hidden_units, ]\n",
    "        Bias for input layer-> hidden layer.\n",
    "    b2 : array, shape = [n_output_units, ]\n",
    "        Bias for hidden layer -> output layer.\n",
    "\n",
    "    \"\"\"\n",
    "    # Points: 2.0\n",
    "    def __init__(self, n_output, n_features, n_hidden=30,\n",
    "                 l2=0.0, epochs=50, lr=0.001, batchsize=1):\n",
    "        self.n_output = n_output\n",
    "        self.n_features = n_features\n",
    "        self.n_hidden = n_hidden\n",
    "        self.l2 = l2\n",
    "        self.epochs = epochs\n",
    "        self.lr = lr\n",
    "        self.batchsize = batchsize\n",
    "        #TODO Initialize weights and biases with np.random.uniform or np.random.normal and specify the shape\n",
    "        self.w1 = np.random.uniform(0, 1, [self.n_features, self.n_hidden])\n",
    "        self.w2 = np.random.uniform(0, 1, [self.n_hidden, self.n_output])\n",
    "        self.b1 = np.random.uniform(0, 1, [self.n_hidden,])\n",
    "        self.b2 = np.random.uniform(0, 1, [self.n_output,])\n",
    "        \n",
    "    # Points: 0.5\n",
    "    def sigmoid(self, z):\n",
    "        \"\"\"Compute sigmoid function\"\"\"\n",
    "        #TODO Implement\n",
    "        return 1.0 / (1.0 + np.exp(-z))\n",
    "\n",
    "    # Points: 0.5\n",
    "    def sigmoid_gradient(self, z):\n",
    "        \"\"\"Compute gradient of the sigmoid function\"\"\"\n",
    "        #TODO Implement\n",
    "        sig = self.sigmoid(z)\n",
    "        return sig * (1 - sig)\n",
    "    \n",
    "    # Points: 1.0\n",
    "    def softmax(self, z):\n",
    "        \"\"\"Compute softmax function.\n",
    "        Implement a stable version which \n",
    "        takes care of overflow and underflow.\n",
    "        \"\"\"        \n",
    "        #TODO Implement\n",
    "        exp_z = np.exp(z - np.max(z))\n",
    "        return exp_z / np.sum(exp_z)\n",
    "    \n",
    "    def softmax_gradient(self,z):\n",
    "        \"\"\"\n",
    "        Compute gradient of the softmax function\n",
    "        \"\"\"\n",
    "        SM = z.reshape((-1, 1))\n",
    "        return np.diag(z) - np.dot(SM, SM.T)\n",
    "    \n",
    "    # Points: 2.0\n",
    "    def forward(self, X):\n",
    "        \"\"\"Compute feedforward step\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, shape = [n_samples, n_features]\n",
    "            \n",
    "        Returns\n",
    "        ----------\n",
    "        z2 : array,\n",
    "            Input of the hidden layer.\n",
    "        a2 : array,\n",
    "            Output of the hidden layer.\n",
    "        z3 : array,\n",
    "            Input of the output layer.\n",
    "        a3 : array,\n",
    "            Output of the output layer.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        z2 = X\n",
    "        a2 = self.sigmoid(np.matmul(z2, self.w1) + self.b1)\n",
    "        z3 = a2\n",
    "        a3 = self.softmax(np.matmul(z3, self.w2) + self.b2)\n",
    "        \n",
    "        return z2, a2, z3, a3\n",
    "        \n",
    "    # Points: 0.5\n",
    "    def L2_regularization(self, lambd):\n",
    "        \"\"\"Implement L2-regularization loss\"\"\"\n",
    "        #TODO Implement\n",
    "        #I add in the loss function\n",
    "        \n",
    "    # Points: 2.0\n",
    "    def loss(self, y_enc, output, epsilon=1e-12):\n",
    "        \"\"\"Implement the cross-entropy loss.\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "        \n",
    "        output : array, output of the output layer\n",
    "        \n",
    "        epsilon: used to turn log(0) into log(epsilon)\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        cost : float, total loss.\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        pred = np.clip(output, epsilon, 1. - epsilon)\n",
    "        N = pred.shape[0]\n",
    "        cost = -np.sum(y_enc * np.log(pred + epsilon)) / N\n",
    "        if self.l2:\n",
    "            cost = cost + self.l2 * (np.sum(self.w1**2) + np.sum(self.w2**2))\n",
    "        return cost\n",
    "        \n",
    "        \n",
    "    # Points: 4.0\n",
    "    def compute_gradient(self, X, a2, a3, z2, y_enc):\n",
    "        \"\"\" Compute gradient using backpropagation.\n",
    "\n",
    "        Parameters\n",
    "        ------------\n",
    "        X : array, Input.\n",
    "        a2 : array, output of the hidden layer.\n",
    "        a3 : array, output of the output layer.\n",
    "        z2 : array, input of the hidden layer.\n",
    "        y_enc : array, one-hot encoded labels.\n",
    "\n",
    "        Returns\n",
    "        ---------\n",
    "        grad1 : array, Gradient of the weight matrix w1.\n",
    "        grad2 : array, Gradient of the weight matrix w2.\n",
    "        grad3 : array, Gradient of the bias vector b1.\n",
    "        grad4 : array, Gradient of the bias vector b2.\n",
    "        \"\"\"\n",
    "        #TODO Implement\n",
    "        N = y_enc.shape[0]\n",
    "        grad4 = a3[range(idx.shape[0]),idx] - 1\n",
    "        grad3 = 1\n",
    "        grad2 = 1\n",
    "        grad1 = 1\n",
    "\n",
    "        return grad1, grad2, grad3, grad4\n",
    "        \n",
    "    # Points: 1.0\n",
    "    def inference(self, X):\n",
    "        \"\"\"Predict class labels\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        y_pred : array, Predicted labels.\n",
    "\n",
    "        \"\"\"\n",
    "        # TODO Implement\n",
    "        _, _, _, y_pred = self.forward(X)\n",
    "        return y_pred\n",
    "    \n",
    "    \n",
    "    def shuffle_train_data(self, X, Y):\n",
    "        \"\"\"called after each epoch\"\"\"\n",
    "        perm = np.random.permutation(Y.shape[0])\n",
    "        X_shuf = X[perm]\n",
    "        Y_shuf = Y[perm]\n",
    "        return X_shuf, Y_shuf\n",
    "    \n",
    "    def to_one_hot(self, Y, num_class):\n",
    "        return np.eye(Y.shape[0], num_class)[Y]\n",
    "    \n",
    "    # Points: 2.0\n",
    "    def train(self, X_train, Y_train, verbose=False):\n",
    "        \"\"\" Fit the model.\n",
    "\n",
    "        Parameters\n",
    "        -----------\n",
    "        X : array, Input.\n",
    "        y : array, Ground truth class labels.\n",
    "        verbose : bool, Print the training progress\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        self\n",
    "\n",
    "        \"\"\"\n",
    "        #TODO Initialization\n",
    "        self.cost_ = []\n",
    "        \n",
    "\n",
    "        for i in range(self.epochs):\n",
    "        \n",
    "            if verbose:\n",
    "                print('\\nEpoch: %d/%d' % (i+1, self.epochs))\n",
    "\n",
    "            bs = 0\n",
    "            nsamples = X_train.shape[0]\n",
    "            # perform a single epoch\n",
    "            while bs < nsamples:\n",
    "                X = X_train[bs: bs+ self.batchsize]\n",
    "                Y = Y_train[bs: bs+ self.batchsize]\n",
    "                # feedforward and loss computation\n",
    "                z2, a2, z3, a3 = self.forward(X)\n",
    "                y_enc = self.to_one_hot(Y, self.n_output)\n",
    "                # compute gradient via backpropagation and update the weights\n",
    "                grad1, grad2, grad3, grad4 = self.compute_gradient(X, a2, a3, z2, y_enc)\n",
    "                self.w1 = self.w1 - self.lr * grad1\n",
    "                self.w2 = self.w2 - self.lr * grad2\n",
    "                self.b1 = self.b1 - self.lr * grad3\n",
    "                self.b2 = self.b2 - self.lr * grad4\n",
    "                # cost for one iteration\n",
    "                _, _, _, a3 = self.forward(X)\n",
    "                cost = self.loss(y_enc, a3)\n",
    "                self.cost_itera.append(cost)\n",
    "                bs = bs + self.batchsize\n",
    "            # cost for one epoch\n",
    "            _, _, _, a3 = self.forward(X_train)\n",
    "            y_enc = self.to_one_hot(Y_train, self.n_output)\n",
    "            cost = self.loss(y_enc, a3)\n",
    "            print('\\nloss: %f' % (cost))\n",
    "            self.cost_.append(cost)\n",
    "            #shuffle the data\n",
    "            X_train, Y_train = self.shuffle_train_data(X_train, Y_train)\n",
    "\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $15.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = Fully_connected_Neural_Network(n_output=10, \n",
    "                                    n_features=X_trainval.shape[1], \n",
    "                                    n_hidden=50, \n",
    "                                    l2=0.1, \n",
    "                                    epochs=1000, \n",
    "                                    lr=0.001,\n",
    "                                    batchsize=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn.train(X_trainval, Y_trainval, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Plot the training error for every iteration\n",
    "# in every epoch\n",
    "\n",
    "x = range(nn.epochs)\n",
    "nn.cost_itera.shape[0]\n",
    "bs = 0\n",
    "iteras = np.floor(X_trainval.shape[0] / nn.batchsize)\n",
    "for i in range(x.shape[0]):\n",
    "    y = nn.cost_itera[bs, bs+iteras]\n",
    "    plt.plot(x,y)\n",
    "    bs = bs + iteras\n",
    "# TODO Implement\n",
    "plt.plot(range(self.epochs), self.cost_)\n",
    "plt.xlabel(\"iteration\")\n",
    "plt.ylabel(\"error\")\n",
    "plt.title('training error')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the training error in every epoch\n",
    "# TODO Implement\n",
    "plt.plot(x, nn.cost_)\n",
    "plt.xlabel(\"epoch\")\n",
    "plt.ylabel(\"training error\")\n",
    "plt.title(\"training error\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $1.0$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute Training Accuracy\n",
    "def accuracy(nn, X, Y):\n",
    "    Y_one_hot = nn.inference(X)\n",
    "    Y_labels = np.argmax(Y_one_hot, axis=1)\n",
    "    acc = np.sum(np.equal(Y_labels, Y).astype(np.float64))\n",
    "    return acc\n",
    "\n",
    "# TODO Implement\n",
    "accuracy(nn, X_trainval, Y_trainval)\n",
    "print('Training accuracy: %.2f%%' % (acc * 100))\n",
    "\n",
    "# Compute Test Accuracy\n",
    "# TODO Implement\n",
    "accuracy(nn, X_test, Y_test)\n",
    "print('Test accuracy: %.2f%%' % (acc * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "**Points:** $0.0$ of $0.5$\n",
    "**Comments:** None\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submission instructions\n",
    "You should provide a single Jupyter notebook (.ipynb file) as the solution. Put the names and student ids of your team members below. **Make sure to submit only 1 solution to only 1 tutor.**\n",
    "\n",
    "- Pengqiu Li 2575746\n",
    "- Wentao Liu 2572849\n",
    "- Fei Chen 2567445"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Points: 0.0 of 30.0 points"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
